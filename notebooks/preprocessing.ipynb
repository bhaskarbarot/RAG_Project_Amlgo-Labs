{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f4beaf",
   "metadata": {},
   "source": [
    "#### Date 30th july 2025 Project Rag for AMLGO LABS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eff5b978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# import main libraries \n",
    "import PyPDF2\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02f55df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed page 1\n",
      "Processed page 2\n",
      "Processed page 3\n",
      "Processed page 4\n",
      "Processed page 5\n",
      "Processed page 6\n",
      "Processed page 7\n",
      "Processed page 8\n",
      "Processed page 9\n",
      "Processed page 10\n",
      "Processed page 11\n",
      "Processed page 12\n",
      "Processed page 13\n",
      "Processed page 14\n",
      "Processed page 15\n",
      "Processed page 16\n",
      "Processed page 17\n",
      "Processed page 18\n",
      "Processed page 19\n",
      "Processed page 20\n",
      "\n",
      " Document extracted\n",
      "Total characters: 68,669\n",
      "First 500 characters:\n",
      "User Agreement  \n",
      "1. Introduction  \n",
      "This User Agreement, the Mobile Application Terms of Use , and all policies and additional terms \n",
      "posted on and in our sites, applications, tools, and services (collectively \"Services\") set out the terms \n",
      "on which eBay offers you access to and use of our Services. You can find an overview of our policies \n",
      "here . The Mobile Application Terms of Use, all policies, and additional terms posted on and in our \n",
      "Services are incorporated into this User Agreement. You a...\n"
     ]
    }
   ],
   "source": [
    "# extract text from pdf\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page_num, page in enumerate(pdf_reader.pages):\n",
    "                text += page.extract_text()\n",
    "                print(f\"Processed page {page_num + 1}\")\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return \"\"\n",
    "pdf_path = r\"/home/petpooja/Documents/amlgo/data/AI Training Document.pdf\"\n",
    "raw_text = extract_text_from_pdf(pdf_path)\n",
    "print(f\"\\n Document extracted\")\n",
    "print(f\"Total characters: {len(raw_text):,}\")\n",
    "print(f\"First 500 characters:\\n{raw_text[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06462065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaned!\n",
      "Cleaned length: 67,504 characters\n",
      "Reduction: 1,165 characters removed\n",
      "\n",
      "Cleaned sample:\n",
      "User Agreement 1. Introduction This User Agreement, the Mobile Application Terms of Use , and all policies and additional terms posted on and in our sites, applications, tools, and services (collectively \"Services\") set out the terms on which eBay offers you access to and use of our Services. You can find an overview of our policies here . The Mobile Application Terms of Use, all policies, and additional terms posted on and in our Services are incorporated into this User Agreement. You agree to ...\n"
     ]
    }
   ],
   "source": [
    "# clean the text using Regullar expression\n",
    "def clean_text(text: str) -> str:\n",
    "\n",
    "\n",
    "    # Remove extra whitespace and normalize\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "\n",
    "    \n",
    "    # Remove special characters but keep essential punctuation\n",
    "    text = re.sub(r'[^\\w\\s.,!?;:()\\-\\'\\\"]+', '', text)\n",
    "\n",
    "\n",
    "    \n",
    "    # Remove multiple consecutive punctuation\n",
    "    text = re.sub(r'([.,!?;]){2,}', r'\\1', text)\n",
    "    \n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Clean the extracted text\n",
    "cleaned_text = clean_text(raw_text)\n",
    "print(f\"Text cleaned!\")\n",
    "print(f\"Cleaned length: {len(cleaned_text):,} characters\")\n",
    "print(f\"Reduction: {len(raw_text) - len(cleaned_text):,} characters removed\")\n",
    "print(f\"\\nCleaned sample:\\n{cleaned_text[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389f07c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello world.', 'How are you?', 'This is a test.']\n"
     ]
    }
   ],
   "source": [
    "## check it is working or not into my system \n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Hello world. How are you? This is a test.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f42f48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating intelligent chunks\n",
      "\n",
      "Chunking Results:\n",
      "Total chunks created: 306\n",
      "Average chunk length: 508.8 characters\n",
      "Length range: 2 - 2131 characters\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "Chunk 1 (ID: 0):\n",
      "Length: 17 chars, Sentences: 1\n",
      "Text: User Agreement 1....\n",
      "\n",
      "Chunk 2 (ID: 1):\n",
      "Length: 275 chars, Sentences: 1\n",
      "Text: Introduction This User Agreement, the Mobile Application Terms of Use , and all policies and additional terms posted on and in our sites, applications, tools, and services (collectively \"Services\") se...\n",
      "\n",
      "Chunk 3 (ID: 2):\n",
      "Length: 192 chars, Sentences: 2\n",
      "Text: You can find an overview of our policies here . The Mobile Application Terms of Use, all policies, and additional terms posted on and in our Services are incorporated into this User Agreement....\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from typing import List\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Intelligent chunking using spaCy\n",
    "def sentence_aware_chunking(text: str, target_chunk_size: int = 200, overlap_sentences: int = 2) -> List[dict]:\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_sentences = []\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
    "        \n",
    "        if len(test_chunk) <= target_chunk_size or not current_chunk:\n",
    "            current_chunk = test_chunk\n",
    "            current_sentences.append(sentence)\n",
    "        else:\n",
    "            # Save current chunk\n",
    "            if current_chunk:\n",
    "                chunk_data = {\n",
    "                    'id': len(chunks),\n",
    "                    'text': current_chunk.strip(),\n",
    "                    'length': len(current_chunk),\n",
    "                    'sentence_count': len(current_sentences),\n",
    "                    'start_sentence': i - len(current_sentences),\n",
    "                    'end_sentence': i - 1\n",
    "                }\n",
    "                chunks.append(chunk_data)\n",
    "            # Create new chunk with overlap\n",
    "            if len(current_sentences) > overlap_sentences:\n",
    "                overlap_text = \" \".join(current_sentences[-overlap_sentences:])\n",
    "                current_chunk = overlap_text + \" \" + sentence\n",
    "                current_sentences = current_sentences[-overlap_sentences:] + [sentence]\n",
    "            else:\n",
    "                current_chunk = sentence\n",
    "                current_sentences = [sentence]\n",
    "\n",
    "    # Add final chunk\n",
    "    if current_chunk:\n",
    "        chunk_data = {\n",
    "            'id': len(chunks),\n",
    "            'text': current_chunk.strip(),\n",
    "            'length': len(current_chunk),\n",
    "            'sentence_count': len(current_sentences),\n",
    "            'start_sentence': len(sentences) - len(current_sentences),\n",
    "            'end_sentence': len(sentences) - 1\n",
    "        }\n",
    "        chunks.append(chunk_data)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# ---- RUN THE FUNCTION ----\n",
    "print(\"Creating intelligent chunks\")\n",
    "chunks_data = sentence_aware_chunking(cleaned_text, target_chunk_size=250, overlap_sentences=2)\n",
    "\n",
    "# Display statistics\n",
    "total_chars = sum(chunk['length'] for chunk in chunks_data)\n",
    "avg_length = total_chars / len(chunks_data)\n",
    "\n",
    "print(f\"\\nChunking Results:\")\n",
    "print(f\"Total chunks created: {len(chunks_data)}\")\n",
    "print(f\"Average chunk length: {avg_length:.1f} characters\")\n",
    "print(f\"Length range: {min(c['length'] for c in chunks_data)} - {max(c['length'] for c in chunks_data)} characters\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(f\"\\nSample chunks:\")\n",
    "for i in range(min(3, len(chunks_data))):\n",
    "    chunk = chunks_data[i]\n",
    "    print(f\"\\nChunk {i+1} (ID: {chunk['id']}):\")\n",
    "    print(f\"Length: {chunk['length']} chars, Sentences: {chunk['sentence_count']}\")\n",
    "    print(f\"Text: {chunk['text'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbfb5836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chunks saved successfully!\n",
      "Files created:\n",
      "  - ../chunks/processed_chunks.json\n",
      "  - ../chunks/chunk_texts.json\n",
      "\n",
      " Document processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Save chunks with metadata\n",
    "import pandas as pd\n",
    "chunks_to_save = {\n",
    "    'metadata': {\n",
    "        'source_document': 'AI_Training_Document.pdf',\n",
    "        'processing_date': str(pd.Timestamp.now()),\n",
    "        'total_chunks': len(chunks_data),\n",
    "        'avg_chunk_length': avg_length,\n",
    "        'chunking_method': 'sentence_aware_with_overlap'\n",
    "    },\n",
    "    'chunks': chunks_data\n",
    "}\n",
    "\n",
    "# Ensure chunks directory exists\n",
    "os.makedirs('../chunks', exist_ok=True)\n",
    "\n",
    "# Save detailed chunk data\n",
    "with open('../chunks/processed_chunks.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks_to_save, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save simple chunk texts for embedding (backward compatibility)\n",
    "chunk_texts = [chunk['text'] for chunk in chunks_data]\n",
    "with open('../chunks/chunk_texts.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunk_texts, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\" Chunks saved successfully!\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"  - ../chunks/processed_chunks.json\")\n",
    "print(f\"  - ../chunks/chunk_texts.json\")\n",
    "print(f\"\\n Document processing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
