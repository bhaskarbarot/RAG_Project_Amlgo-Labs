{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d76bc8",
   "metadata": {},
   "source": [
    "#### Date 30th july 2025 Project Rag for AMLGO LABS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b046440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up language model and RAG pipeline...\n",
      "Selected model: microsoft/DialoGPT-medium\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from typing import List, Tuple, Iterator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setting up language model and RAG pipeline...\")\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "print(f\"Selected model: {model_name}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2852ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading tokenizer and model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215f98fd24154f6cb4f7aaef8ad7552f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41c6c01a9a04c8c998c56a97533e433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87941638bb44e8398f74ffbf0e9bc7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4176ad5f685f4bf48d5ed92507d28c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f403015db1b4112ab4c0d29433c7cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/863M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c656250ec6f41bd849c5f4378325458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d055b52962d419392649a4d82f7993d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/863M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "print(\" Loading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "# Configure tokenizer\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d64dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created document_processor.py\n"
     ]
    }
   ],
   "source": [
    "# Create src/__init__.py\n",
    "os.makedirs('../src', exist_ok=True)\n",
    "with open('../src/__init__.py', 'w') as f:\n",
    "    f.write('# RAG Chatbot Source Package\\n')\n",
    "\n",
    "# Create document_processor.py\n",
    "doc_processor_code = '''\"\"\"Document processing utilities\"\"\"\n",
    "import PyPDF2\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF file\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text()\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error reading PDF: {e}\")\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean extracted text\"\"\"\n",
    "        text = re.sub(r'\\\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\\\w\\\\s.,!?;:()\\\\-\\\\'\\\\\\\"]+', '', text)\n",
    "        text = re.sub(r'([.,!?;]){2,}', r'\\\\1', text)\n",
    "        return text.strip()\n",
    "'''\n",
    "\n",
    "with open('../src/document_processor.py', 'w') as f:\n",
    "    f.write(doc_processor_code)\n",
    "\n",
    "print(\"Created document_processor.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2afe2269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Created retriever.py\n"
     ]
    }
   ],
   "source": [
    "# Create retriever.py\n",
    "retriever_code = '''\"\"\"Document retrieval using FAISS and sentence transformers\"\"\"\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Tuple\n",
    "\n",
    "class DocumentRetriever:\n",
    "    def __init__(self, index_path: str, metadata_path: str, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        \"\"\"Initialize retriever with FAISS index and metadata\"\"\"\n",
    "        self.embedding_model = SentenceTransformer(model_name)\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        \n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        self.chunks = [chunk['text'] for chunk in self.metadata['chunks']]\n",
    "        \n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Retrieve most relevant chunks for the query\"\"\"\n",
    "        # Encode query with normalization\n",
    "        query_embedding = self.embedding_model.encode(\n",
    "            [query], \n",
    "            normalize_embeddings=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # Search in FAISS index\n",
    "        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)\n",
    "        \n",
    "        # Return chunks with scores\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < len(self.chunks):\n",
    "                results.append((self.chunks[idx], float(score)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_chunk_metadata(self, chunk_text: str) -> dict:\n",
    "        \"\"\"Get metadata for a specific chunk\"\"\"\n",
    "        for chunk in self.metadata['chunks']:\n",
    "            if chunk['text'] == chunk_text:\n",
    "                return chunk\n",
    "        return {}\n",
    "'''\n",
    "\n",
    "with open('../src/retriever.py', 'w') as f:\n",
    "    f.write(retriever_code)\n",
    "\n",
    "print(\" Created retriever.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6a74309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Created generator.py\n"
     ]
    }
   ],
   "source": [
    "# Create generator.py\n",
    "generator_code = '''\"\"\"Response generation using transformers\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from typing import Iterator\n",
    "\n",
    "class ResponseGenerator:\n",
    "    def __init__(self, model_name: str = \"microsoft/DialoGPT-medium\"):\n",
    "        \"\"\"Initialize response generator\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def generate(self, prompt: str, max_new_tokens: int = 150) -> str:\n",
    "        \"\"\"Generate response from prompt\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer.encode(prompt, return_tensors='pt', truncate=True, max_length=512)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode only the new tokens\n",
    "            generated_text = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "            return generated_text.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def generate_streaming(self, prompt: str, max_new_tokens: int = 150) -> Iterator[str]:\n",
    "        \"\"\"Generate streaming response\"\"\"\n",
    "        response = self.generate(prompt, max_new_tokens)\n",
    "        \n",
    "        # Simulate streaming by yielding words\n",
    "        words = response.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if i == 0:\n",
    "                yield word\n",
    "            else:\n",
    "                yield \" \" + word\n",
    "'''\n",
    "\n",
    "with open('../src/generator.py', 'w') as f:\n",
    "    f.write(generator_code)\n",
    "\n",
    "print(\" Created generator.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99b0ee66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created pipeline.py\n",
      "All source files created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline.py\n",
    "pipeline_code = '''\"\"\"Complete RAG pipeline combining retrieval and generation\"\"\"\n",
    "from .retriever import DocumentRetriever\n",
    "from .generator import ResponseGenerator\n",
    "from typing import List, Tuple, Iterator\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, index_path: str, metadata_path: str, model_name: str = \"microsoft/DialoGPT-medium\"):\n",
    "        \"\"\"Initialize complete RAG pipeline\"\"\"\n",
    "        print(\"Initializing RAG pipeline...\")\n",
    "        self.retriever = DocumentRetriever(index_path, metadata_path)\n",
    "        self.generator = ResponseGenerator(model_name)\n",
    "        print(\"RAG pipeline initialized!\")\n",
    "    \n",
    "    def create_prompt(self, query: str, retrieved_chunks: List[str]) -> str:\n",
    "        \"\"\"Create structured prompt for RAG\"\"\"\n",
    "        context = \"\\\\n\\\\n\".join([f\"Context {i+1}: {chunk}\" for i, chunk in enumerate(retrieved_chunks)])\n",
    "        \n",
    "        prompt = f\"\"\"Based on the eBay User Agreement information provided below, answer the user's question accurately and concisely.\n",
    "\n",
    "Context Information:\n",
    "{context}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Instructions:\n",
    "- Answer based only on the provided context\n",
    "- Be specific and accurate\n",
    "- If the context doesn't contain enough information, say so\n",
    "- Keep the response concise but complete\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def query(self, user_query: str, top_k: int = 3) -> Tuple[str, List[str]]:\n",
    "        \"\"\"Process user query and return response with sources\"\"\"\n",
    "        # Retrieve relevant chunks\n",
    "        retrieved_results = self.retriever.retrieve(user_query, top_k)\n",
    "        retrieved_chunks = [chunk for chunk, score in retrieved_results]\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = self.create_prompt(user_query, retrieved_chunks)\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.generator.generate(prompt, max_new_tokens=200)\n",
    "        \n",
    "        return response, retrieved_chunks\n",
    "    \n",
    "    def query_streaming(self, user_query: str, top_k: int = 3) -> Tuple[Iterator[str], List[str]]:\n",
    "        \"\"\"Process user query and return streaming response with sources\"\"\"\n",
    "        # Retrieve relevant chunks\n",
    "        retrieved_results = self.retriever.retrieve(user_query, top_k)\n",
    "        retrieved_chunks = [chunk for chunk, score in retrieved_results]\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = self.create_prompt(user_query, retrieved_chunks)\n",
    "        \n",
    "        # Generate streaming response\n",
    "        response_stream = self.generator.generate_streaming(prompt, max_new_tokens=200)\n",
    "        \n",
    "        return response_stream, retrieved_chunks\n",
    "'''\n",
    "\n",
    "with open('../src/pipeline.py', 'w') as f:\n",
    "    f.write(pipeline_code)\n",
    "\n",
    "print(\"Created pipeline.py\")\n",
    "print(\"All source files created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5314325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to sys.path so `src` can be imported\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "994fe1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RAG pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG pipeline initialized!\n",
      "\\nTesting pipeline with query: 'What are eBay's return policies?'\n",
      "\\nGenerated Response:\n",
      "Error generating response: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'truncate'\n",
      "\\nRetrieved Sources (2):\n",
      "\\nSource 1: Where settings have been set to automatically accept requests for returns or replacements, an eBay -generated return shipping label will be provided to your buyer. You agree to comply with our returns...\n",
      "\\nSource 2: 14. Additional Terms Returns and cancellations for sellers Sellers can create rules to automate replacements, returns, and refunds under certain circumstances. For all new sellers, in listings where r...\n",
      "\\nPipeline test successful!\n"
     ]
    }
   ],
   "source": [
    "# Test the complete pipeline\n",
    "try:\n",
    "    from src.pipeline import RAGPipeline\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = RAGPipeline(\n",
    "        index_path=\"../vectordb/document_index.faiss\",\n",
    "        metadata_path=\"../vectordb/metadata.json\",\n",
    "        model_name=model_name\n",
    "    )\n",
    "    \n",
    "    # Test query\n",
    "    test_query = \"What are eBay's return policies?\"\n",
    "    print(f\"\\\\nTesting pipeline with query: '{test_query}'\")\n",
    "    \n",
    "    response, sources = pipeline.query(test_query, top_k=2)\n",
    "    \n",
    "    print(f\"\\\\nGenerated Response:\")\n",
    "    print(f\"{response}\")\n",
    "    \n",
    "    print(f\"\\\\nRetrieved Sources ({len(sources)}):\")\n",
    "    for i, source in enumerate(sources, 1):\n",
    "        print(f\"\\\\nSource {i}: {source[:200]}...\")\n",
    "    \n",
    "    print(\"\\\\nPipeline test successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Pipeline test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03407746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
